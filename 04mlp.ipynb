{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04mlp.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMmd4i0Is0jrObhCJT02xcr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oyoECoEwMQ-u","executionInfo":{"status":"ok","timestamp":1642224333820,"user_tz":-540,"elapsed":240,"user":{"displayName":"あいうえおTeamF","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaCBWzkYqZqRQYxfC4DkLDypbRuIni4SXpMFm1Ug=s64","userId":"09495778697115388516"}},"outputId":"bd3faa6a-fc3c-4b03-9223-fcd8e83c058e"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss:  3.003\n","epoch: 11, loss:  2.884\n","epoch: 21, loss:  2.840\n","epoch: 31, loss:  2.815\n","epoch: 41, loss:  2.799\n","epoch: 51, loss:  2.789\n","epoch: 61, loss:  2.783\n","epoch: 71, loss:  2.779\n","epoch: 81, loss:  2.777\n","epoch: 91, loss:  2.775\n","epoch: 100, loss:  2.774\n","[0 0] =>  0.513\n","[0 1] =>  0.485\n","[1 0] =>  0.519\n","[1 1] =>  0.491\n"]}],"source":["import numpy as np\n","\n","class LogisticRegression(object):\n","  '''\n","  ロジスティック回帰\n","  '''\n","  def __init__(self, input_dim):\n","    self.input_dim = input_dim\n","    self.w = np.random.normal(size=(input_dim,))\n","    self.b = 0.\n","\n","  def __call__(self, x):\n","    return self.forward(x)\n","  \n","  def forward(self, x):\n","    return sigmoid(np.matmul(x, self.w) + self.b)\n","  \n","  def compute_gradients(self, x, t):\n","    y = self.forward(x)\n","    delta = y - t\n","    dw = np.matmul(x.T, delta)\n","    db = np.matmul(np.ones(x.shape[0]), delta)\n","    return dw, db\n","\n","def sigmoid(x):\n","  return 1 / (1 + np.exp(-x))\n","\n","if __name__ == '__main__':\n","  np.random.seed(1234)\n","\n","  '''\n","  1. データの準備\n","  '''\n","  #XOR\n","  x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","  t = np.array([0 , 1 , 1 , 0])\n","  '''\n","  2. モデルの構築\n","  '''\n","  model = LogisticRegression(input_dim=2)\n","  '''\n","  3. モデルの学習\n","  '''\n","  def compute_loss(t, y):\n","    return (-t * np.log(y) - (1 - t) * np.log(1 - y)).sum()\n","  \n","  def train_step(x, t):\n","    dw, db = model.compute_gradients(x, t)\n","    model.w = model.w - 0.1 * dw\n","    model.b = model.b - 0.1 * db\n","    loss = compute_loss(t, model(x))\n","    return loss\n","\n","  epochs = 100\n","\n","  for epoch in range(epochs):\n","    train_loss = train_step(x, t) #パッチ学習\n","\n","    if epoch % 10 == 0 or epoch == epochs - 1:\n","      print('epoch: {}, loss: {: .3f}' .format(epoch+1, train_loss))\n","  '''\n","  4. モデルの評価\n","  '''\n","  for input in x :\n","    print('{} => {: .3f}'.format(input, model(input)))\n"]},{"cell_type":"code","source":["import numpy as np\n","\n","class MLP(object):\n","  '''\n","  多層パーセプトロン\n","  '''\n","  def __init__(self, input_dim, hidden_dim, output_dim):\n","    '''\n","    引数:\n","          input_dim: 入力層の次元\n","          hidden_dim: 隠れ層の次元\n","          output_dim: 出力層の次元\n","    '''\n","    self.l1 = Layer(input_dim=input_dim,\n","                    output_dim=hidden_dim,\n","                    activation=sigmoid,\n","                    dactivation=dsigmoid)\n","    \n","    self.l２ = Layer(input_dim=hidden_dim,\n","                    output_dim=output_dim,\n","                    activation=sigmoid,\n","                    dactivation=dsigmoid)\n","    \n","    self.layers = [self.l1, self.l2]\n","\n","  def __call__(self, x):\n","    return self.forward(x)\n","\n","  def forward(self, x):\n","    h = self.l1(x)\n","    y = self.l2(h)\n","    return y\n","\n","def sigmoid(x):\n","  return 1 / (1 + np.exp(-x))\n","\n","def dsigmoid(x):\n","  return sigmoid(x) * (1 - sigmoid(x))\n","\n","class Layer(object):\n","  def __init__(self, input_dim, output_dim, activation, dactivation):\n","    '''\n","    インスタンス変数:\n","      W: 重み\n","      b: バイアス\n","      activation: 活性化関数\n","      dactivation: 活性化関数の微分\n","    '''\n","    self.W = np.random.normal(size=(input_dim, output_dim))\n","    self.b = np.zeros(output_dim)\n","\n","    self.activation = activation\n","    self.dactivation = dactivation\n","  \n","  def __call__(self, x):\n","    return self.forward(x)\n","  \n","  def forward(self, x):\n","    self._input = x\n","    self._pre_activation = np.matmul(x, self.W) + self.b\n","    return self.activation(self._pre_activation)\n","\n","  def backward(self, delta, W):\n","    delta = self.dactivation(self._pre_activation) * np.matmul(delta, W.T)\n","    return delta\n","\n","  def compute_gradients(self, delta):\n","    dW = np.matmul(self._input.T, delta)\n","    db = np.matmul(np.ones(self._input.shape[0]), delta)\n","    return dW, db\n","\n","if __name__ == '__main__':\n","  np.random.seed(123)\n","\n","  '''\n","  1. データの準備\n","  '''\n","  #XOR\n","  x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","  t = np.array([[0], [1], [1], [0]])\n","  '''\n","  2. モデルの構築\n","  '''\n","  model = MLP(2, 2, 1)\n","  '''\n","  3. モデルの学習\n","  '''\n","  def compute_loss(t, y):\n","    return (-t * np.log(y) - (1 - t) * np.log(1 - y)).sum()\n","  \n","  def train_step(x, t):\n","    y = model(x)\n","    for i, layer in enumerate(model.layers[: : -1]):\n","      if i == 0:\n","        delta = y - t\n","      else:\n","        delta = layer.backward(delta, W)\n","\n","      dW, db = layer.compute_gradients(delta)\n","      layer.W = layer.W - 0.1 * dW\n","      layer.b = layer.b - 0.1 * db\n","\n","      W = layer.W\n","    \n","    loss = compute_loss(t, y)\n","    return loss\n","  \n","  epochs = 1000\n","\n","  for epoch in range(epochs):\n","    train_loss = train_step(x, t)\n","\n","    if epoch % 100 == 0 or epoch == epochs - 1:\n","      print('epoch: {}, loss: {:.3f}'\n","      .format(epoch+1,train_loss))\n","\n","  '''\n","  4. モデルの評価\n","  '''\n","  for input in x:\n","    print('{} => {:.3f}'.format(input, model(input)[0]))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zuF6aKLLMrGS","executionInfo":{"status":"ok","timestamp":1642227896841,"user_tz":-540,"elapsed":607,"user":{"displayName":"あいうえおTeamF","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgaCBWzkYqZqRQYxfC4DkLDypbRuIni4SXpMFm1Ug=s64","userId":"09495778697115388516"}},"outputId":"28eade91-d163-4838-ef15-03c6419b6a22"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch: 1, loss: 2.940\n","epoch: 101, loss: 2.696\n","epoch: 201, loss: 2.572\n","epoch: 301, loss: 2.403\n","epoch: 401, loss: 2.250\n","epoch: 501, loss: 2.110\n","epoch: 601, loss: 1.849\n","epoch: 701, loss: 1.274\n","epoch: 801, loss: 0.738\n","epoch: 901, loss: 0.470\n","epoch: 1000, loss: 0.336\n","[0 0] => 0.096\n","[0 1] => 0.909\n","[1 0] => 0.944\n","[1 1] => 0.078\n"]}]}]}